# -*- coding: utf-8 -*-
"""talk2anyface_done.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/Daniel-BTCSH/e11f915477baa6b1355f7f6503883f19/talk2anyface_done.ipynb
"""

#判断人脸
!pip install face_recognition

import torch
torch.version.cuda

#@title
!pip install paddlepaddle-gpu==2.4.1.post116 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

#聊天机器人
!pip install --upgrade paddlenlp>=2.0.0rc -i https://pypi.org/simple
from paddlenlp import Taskflow
dialogue = Taskflow("dialogue")
dialogue(['你好呀'])

# 语音
!pip install --upgrade pytest-runner 
!pip install --upgrade paddlespeech

import base64
import librosa
import soundfile as sf

def resample_rate(path,new_sample_rate = 16000):
    signal, sr = librosa.load(path, sr=None)
    wavfile = path.split('/')[-1]
    wavfile = wavfile.split('.')[0]
    file_name = wavfile + '_new.wav'
    new_signal = librosa.resample(signal, sr, new_sample_rate) # 
    #librosa.output.write_wav(file_name, new_signal , new_sample_rate) 
    sf.write(file_name, new_signal, new_sample_rate, subtype='PCM_24')
    print(f'{file_name} has download.')
    return file_name

# 测试语音
from paddlespeech.cli.asr.infer import ASRExecutor
from paddlespeech.cli.tts.infer import TTSExecutor

asr = ASRExecutor()
tts = TTSExecutor()

out_file =tts('你好呀，我是语音测试')
print('tts',out_file)

audio_file=resample_rate(out_file)
result = asr(audio_file=audio_file)
print('asr',result)

# 文生图
!pip install --upgrade ppdiffusers
from ppdiffusers import StableDiffusionPipeline
sd = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
text='girl'
prompt = "a symmetrical portrait of "+text+". intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing "
images = sd(prompt, guidance_scale=7.5,
             num_inference_steps=20,
             height=416, width=416).images

image=images[0]
image.save("test.png")
print(image)

# 唇形
!pip install --upgrade ppgan
from ppgan.apps.wav2lip_predictor import Wav2LipPredictor

wav2lip_predictor = Wav2LipPredictor(face_det_batch_size = 2,wav2lip_batch_size = 16,face_enhancement = True)
wav2lip_predictor.run("test.png", 'output.wav', 'res.mp4')

from paddlespeech.cli.asr.infer import ASRExecutor
from paddlespeech.cli.tts.infer import TTSExecutor
from paddlenlp import Taskflow
from ppgan.apps.wav2lip_predictor import Wav2LipPredictor
from ppgan.apps.first_order_predictor import FirstOrderPredictor
from ppdiffusers import StableDiffusionPipeline

# 可选模型权重
# CompVis/stable-diffusion-v1-4
# runwayml/stable-diffusion-v1-5
# stabilityai/stable-diffusion-2-base （原始策略 512x512）
# stabilityai/stable-diffusion-2 （v-objective 768x768）
# Linaqruf/anything-v3.0
# ......


# from paddlenlp import Taskflow
# text_to_image = Taskflow("text_to_image")

# # https://github.com/JiehangXie/PaddleBoBo/blob/0.1/PaddleTools/GAN.py
sd = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
asr = ASRExecutor()
tts = TTSExecutor()
dialogue = Taskflow("dialogue")
wav2lip_predictor = Wav2LipPredictor(face_det_batch_size = 2,wav2lip_batch_size = 16,face_enhancement = True)

import base64,os
import librosa
import soundfile as sf

def resample_rate(path,new_sample_rate = 16000):
    signal, sr = librosa.load(path, sr=None)
    wavfile = path.split('/')[-1]
    wavfile = wavfile.split('.')[0]
    file_name = wavfile + '_new.wav'
    new_signal = librosa.resample(signal, sr, new_sample_rate) # 
    #librosa.output.write_wav(file_name, new_signal , new_sample_rate) 
    sf.write(file_name, new_signal, new_sample_rate, subtype='PCM_24')
    print(f'{file_name} has download.')
    return file_name

def text2img(text):
  prompt = "a portrait of "+text+".passport photo， intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing "
  image = sd(prompt, 
        guidance_scale=7.5,
        num_inference_steps=20,
        height=416, width=416).images[0]
  return image


def audio2text(input_file):
  input_file=resample_rate(input_file,new_sample_rate = 16000)
  result = asr(audio_file=input_file)
  print('audio2text')
  return result

def text2audio(text):
  out_file =tts(text)
  print('text2audio')
  return out_file

def reply(t):
  data = [t]
  result = dialogue(data)
  print('reply',result[0])
  return result[0]

def wav2lip(input_video,input_audio):
  out_file='video.mp4'
  wav2lip_predictor.run(input_video, input_audio, out_file)
  return out_file


def FOM(source_image,driving_video,output_path):
  output,filename = os.path.split(output_path)
  first_order_predictor = FirstOrderPredictor(output = output,filename = filename, 
                              face_enhancement = True, 
                              ratio = 0.4,
                              relative = True,
                              image_size=512,
                              adapt_scale = True)
  first_order_predictor.run(source_image, driving_video)
  return os.path.join(output,filename)

def write_wav(data, samplerate,wav_file):
  # data, samplerate = sf.read('existing_file.wav')
  sf.write(wav_file, data, samplerate)
  return wav_file



#判断人脸
import face_recognition

def check_face_image(image_path):
  image = face_recognition.load_image_file(image_path)
  face_locations = face_recognition.face_locations(image)
  print("I found {} face(s) in this photograph.".format(len(face_locations)))

  if len(face_locations)==1:
    # top, right, bottom, left = face_locations[0]
    # width=right-left
    # height=bottom-top
    # s=(width*height)/(image.width*image.height)
    # if s>0.4:
    return True
  return False


# from fastapi import FastAPI, APIRouter,Body
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import HTMLResponse

# app = FastAPI()
# api_router = APIRouter()

# origins = ['*']

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=origins,
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

!pip install gradio
import os
import gradio as gr

from google.colab import drive

# 挂载网盘
drive.mount('/content/drive/')  
# 切换路径
os.chdir('/content/drive/MyDrive/data')

def step_by_step(wav_file_input):
    wav_file=write_wav(wav_file_input[1],wav_file_input[0],'./wav_file.wav')
    text=audio2text(wav_file)
    q=reply(text)
    print(q)
    
    input_audio=text2audio(q)
    img=text2img('pretty girl face')
    portrait='portrait.png'
    img.save(portrait)

    if check_face_image(portrait)==False:
      return portrait

    portrait_video=FOM(portrait,'driving_video.mp4','./portrait_video.mp4')
    # portrait_video=portrait
    result=wav2lip(portrait_video,wav_file)
    # os.path.join(os.path.dirname(__file__),  "video.mp4")
    print(result)
    return result

input_audio=gr.Audio(label="录音",type="numpy",source='microphone')

iface = gr.Interface(fn=step_by_step, inputs=[input_audio], outputs=[gr.Video()],
  layout="vertical")

iface.launch(share=True,debug=True)

"""# 新段落"""