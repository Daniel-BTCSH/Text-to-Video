{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-BTCSH/Text-to-Video/blob/main/speechface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUEI42fwDFLr"
      },
      "outputs": [],
      "source": [
        "#判断人脸\n",
        "!pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMj5Yb2zD-r_"
      },
      "outputs": [],
      "source": [
        "!pip install paddlepaddle-gpu==2.4.1.post116 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
        "import paddle.fluid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sYuUNCdyNgS"
      },
      "outputs": [],
      "source": [
        "# 语音\n",
        "!pip install pytest-runner \n",
        "!pip install paddlespeech==1.4.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlJU3oP1CC99"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import base64\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "def resample_rate(path,new_sample_rate = 16000):\n",
        "    signal, sr = librosa.load(path, sr=None)\n",
        "    wavfile = path.split('/')[-1]\n",
        "    wavfile = wavfile.split('.')[0]\n",
        "    file_name = wavfile + '_new.wav'\n",
        "    new_signal = librosa.resample(signal, sr, new_sample_rate) # \n",
        "    #librosa.output.write_wav(file_name, new_signal , new_sample_rate) \n",
        "    sf.write(file_name, new_signal, new_sample_rate, subtype='PCM_24')\n",
        "    print(f'{file_name} has download.')\n",
        "    return file_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "kj-sJbqTqfoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocUmaQDLT7oP"
      },
      "outputs": [],
      "source": [
        "!pip install ppgan\n",
        "!pip uninstall typeguard\n",
        "!pip install typeguard==2.13.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ppgan.apps.wav2lip_predictor import Wav2LipPredictor\n",
        "from paddlespeech.cli.asr.infer import ASRExecutor\n",
        "from paddlespeech.cli.tts.infer import TTSExecutor\n",
        "from paddlenlp import Taskflow\n",
        "from ppgan.apps.wav2lip_predictor import Wav2LipPredictor\n",
        "from ppgan.apps.first_order_predictor import FirstOrderPredictor\n",
        "from ppdiffusers import StableDiffusionPipeline"
      ],
      "metadata": {
        "id": "ncmBmGtUrKIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5rSZZMYB7_z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 可选模型权重\n",
        "# CompVis/stable-diffusion-v1-4\n",
        "# runwayml/stable-diffusion-v1-5\n",
        "# stabilityai/stable-diffusion-2-base （原始策略 512x512）\n",
        "# stabilityai/stable-diffusion-2 （v-objective 768x768）\n",
        "# Linaqruf/anything-v3.0\n",
        "# ......\n",
        "\n",
        "\n",
        "# from paddlenlp import Taskflow\n",
        "# text_to_image = Taskflow(\"text_to_image\")\n",
        "\n",
        "# # https://github.com/JiehangXie/PaddleBoBo/blob/0.1/PaddleTools/GAN.py\n",
        "sd = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "asr = ASRExecutor()\n",
        "tts = TTSExecutor()\n",
        "dialogue = Taskflow(\"dialogue\")\n",
        "wav2lip_predictor = Wav2LipPredictor(face_det_batch_size = 2,wav2lip_batch_size = 16,face_enhancement = True)\n",
        "\n",
        "import base64,os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import hashlib\n",
        "\n",
        "def get_prompt_id(text):\n",
        "  m = hashlib.md5()\n",
        "  m.update(text.encode(\"utf8\"))\n",
        "  return m.hexdigest()\n",
        "\n",
        "\n",
        "def resample_rate(path,new_sample_rate = 16000):\n",
        "    signal, sr = librosa.load(path, sr=None)\n",
        "    wavfile = path.split('/')[-1]\n",
        "    wavfile = wavfile.split('.')[0]\n",
        "    file_name = wavfile + '_new.wav'\n",
        "    new_signal = librosa.resample(signal, sr, new_sample_rate) # \n",
        "    #librosa.output.write_wav(file_name, new_signal , new_sample_rate) \n",
        "    sf.write(file_name, new_signal, new_sample_rate, subtype='PCM_24')\n",
        "    print(f'{file_name} has download.')\n",
        "    return file_name\n",
        "\n",
        "# def text2img(text):\n",
        "#   sd = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",requires_safety_checker=False)\n",
        "#   prompt = \"a portrait of \"+text+\".passport photo， intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing \"\n",
        "#   image = sd(prompt, \n",
        "#         guidance_scale=7.5,\n",
        "#         num_inference_steps=21,\n",
        "#         height=416, width=384).images[0]\n",
        "#   sd=None\n",
        "#   return {\n",
        "#       \"image\":image,\n",
        "#       \"prompt\":prompt,\n",
        "#       \"id\":get_prompt_id(prompt)\n",
        "#   }\n",
        "\n",
        "\n",
        "def audio2text(input_file):\n",
        "  input_file=resample_rate(input_file,new_sample_rate = 16000)\n",
        "  result = asr(audio_file=input_file,device='cpu')\n",
        "  print('audio2text')\n",
        "  return result\n",
        "\n",
        "def text2audio(text):\n",
        "  out_file =tts(text,device='cpu')\n",
        "  print('text2audio')\n",
        "  return out_file\n",
        "\n",
        "def reply(t):\n",
        "  data = [t]\n",
        "  result = dialogue(data)\n",
        "  print('reply',result[0])\n",
        "  return result[0]\n",
        "\n",
        "def wav2lip(input_video,input_audio):\n",
        "  out_file='video.mp4'\n",
        "  wav2lip_predictor.run(input_video, input_audio, out_file)\n",
        "  return out_file\n",
        "\n",
        "\n",
        "def FOM(source_image,driving_video,output_path):\n",
        "  output,filename = os.path.split(output_path)\n",
        "  first_order_predictor = FirstOrderPredictor(output = output,filename = filename, \n",
        "                              face_enhancement = True, \n",
        "                              ratio = 0.4,\n",
        "                              relative = True,\n",
        "                              image_size=512,\n",
        "                              adapt_scale = True)\n",
        "  first_order_predictor.run(source_image, driving_video)\n",
        "  return os.path.join(output,filename)\n",
        "\n",
        "def write_wav(data, samplerate,wav_file):\n",
        "  # data, samplerate = sf.read('existing_file.wav')\n",
        "  sf.write(wav_file, data, samplerate)\n",
        "  return wav_file\n",
        "\n",
        "#判断人脸\n",
        "import face_recognition\n",
        "\n",
        "def check_face_image(image_path):\n",
        "  image = face_recognition.load_image_file(image_path)\n",
        "  face_locations = face_recognition.face_locations(image)\n",
        "  print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
        "\n",
        "  if len(face_locations)==1:\n",
        "    # top, right, bottom, left = face_locations[0]\n",
        "    # width=right-left\n",
        "    # height=bottom-top\n",
        "    # s=(width*height)/(image.width*image.height)\n",
        "    # if s>0.4:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "# from fastapi import FastAPI, APIRouter,Body\n",
        "# from fastapi.middleware.cors import CORSMiddleware\n",
        "# from fastapi.responses import HTMLResponse\n",
        "\n",
        "# app = FastAPI()\n",
        "# api_router = APIRouter()\n",
        "\n",
        "# origins = ['*']\n",
        "\n",
        "# app.add_middleware(\n",
        "#     CORSMiddleware,\n",
        "#     allow_origins=origins,\n",
        "#     allow_credentials=True,\n",
        "#     allow_methods=[\"*\"],\n",
        "#     allow_headers=[\"*\"],\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt9P9E20Ksoz"
      },
      "outputs": [],
      "source": [
        "!pip install gradio==3.22.0\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载网盘\n",
        "drive.mount('/content/drive/')  \n",
        "# 切换路径\n",
        "os.chdir('/content/drive/MyDrive/data')\n",
        "\n",
        "portrait_video=None\n",
        "\n",
        "def create_avatar(portrait_file,file_type):\n",
        "  print('file_type::',file_type)\n",
        "  url=''\n",
        "  # avatar=text2img(text)\n",
        "  #存下来，并索引\n",
        "  # portrait_file=avatar['id']+'.png'\n",
        "  # avatar['image'].save(portrait_file)\n",
        "  # if check_face_image(portrait_file)==False:\n",
        "  #     return portrait_file\n",
        "  portrait_video=FOM(portrait_file,'driving_video.mp4','./portrait_video.mp4')\n",
        "  if file_type=='gif':\n",
        "    portrait_video=convert_mp4_to_gif(portrait_video)\n",
        "    url='data:image/gif;base64,'+encode_base64(portrait_video)\n",
        "  \n",
        "  return '<img src=\"'+url+'\" />'\n",
        "\n",
        "\n",
        "def test(text,wav_file_input,input_type):\n",
        "  portrait_video='./portrait_video.mp4'\n",
        "  if wav_file_input!=None:\n",
        "    wav_file=write_wav(wav_file_input[1],wav_file_input[0],'./wav_file.wav')\n",
        "    text=audio2text(wav_file)\n",
        "  \n",
        "  q=reply(text)\n",
        "  input_audio=text2audio(q)\n",
        "  result=wav2lip(portrait_video,input_audio)\n",
        "  return result\n",
        "\n",
        "def test2(text,wav_file_input,input_type):\n",
        "  portrait_video='./portrait_video.mp4'\n",
        "  if input_type=='wav':\n",
        "    input_audio=wav_file_input\n",
        "  elif input_type=='text':\n",
        "    input_audio=text2audio(text)\n",
        "  \n",
        "  result=wav2lip(portrait_video,input_audio)\n",
        "  return result\n",
        "\n",
        "\n",
        "def encode_base64(file):\n",
        "    with open(file, 'rb') as f:\n",
        "        img_data = f.read()\n",
        "        base64_data = base64.b64encode(img_data)\n",
        "        print(type(base64_data))\n",
        "        # print(base64_data)\n",
        "        # 如果想要在浏览器上访问base64格式图片，需要在前面加上：data:image/jpeg;base64,\n",
        "        base64_str = str(base64_data, 'utf-8')\n",
        "        # print(base64_str)\n",
        "        return base64_str\n",
        "    \n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "def convert_mp4_to_gif(input_file,step=None,duration=None):\n",
        "    '''\n",
        "    传参 :  input_file 视频文件名\n",
        "            output_file gif文件名\n",
        "            duration 每帧图像的停留时间 毫秒ms \n",
        "            step 跳帧,降低采样率,减小gif体积\n",
        "    返回 : 无\n",
        "    '''\n",
        "    output,filename = os.path.split(input_file)\n",
        "    video_capture = cv2.VideoCapture(input_file)\n",
        "    rate=video_capture.get(5)\n",
        "    still_reading, image = video_capture.read()\n",
        "    frame_count = 0\n",
        "    i = 0\n",
        "    frames = []\n",
        "\n",
        "    if duration==None:\n",
        "      duration=1/rate\n",
        "\n",
        "    #单位是秒\n",
        "    print('rate',rate)\n",
        "    if step==None:\n",
        "      if rate>12:\n",
        "        step=rate-12\n",
        "        duration=step/rate\n",
        "      else:\n",
        "        step=1\n",
        "    \n",
        "        \n",
        "    while still_reading:\n",
        "        still_reading, image = video_capture.read()\n",
        "        if not still_reading:\n",
        "            break\n",
        "        if i>step:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(image.copy(),cv2.COLOR_BGR2RGB)))\n",
        "            frame_count += 1\n",
        "            i=0\n",
        "        i+=1\n",
        "    \n",
        "    # v_duration=frame_count/rate\n",
        "    \n",
        "\n",
        "    frame_one = frames[0]\n",
        "    output_file=os.path.join(output,'portrait.gif')\n",
        "    frame_one.save(output_file, format=\"GIF\", append_images=frames[1:],save_all=True, duration=duration, loop=0)\n",
        "    return output_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column():\n",
        "        # avatar\n",
        "        # input_text=gr.Textbox()\n",
        "        input_i=gr.Image(type='filepath')\n",
        "        input_file_type=gr.Radio([\"mp4\", \"gif\"],value='mp4', label=\"文件类型\")\n",
        "        # output_video1=gr.Video(format='mp4')\n",
        "        # output_gif=gr.Image(type=\"numpy\")\n",
        "        \n",
        "        btn = gr.Button(value=\"创建形象\")\n",
        "        btn.click(create_avatar, inputs=[input_i,input_file_type], outputs=[gr.HTML()],api_name='create_avatar')\n",
        "    with gr.Column():\n",
        "        # talking\n",
        "        input_file_type=gr.Radio([\"wav\", \"text\"],value='wav', label=\"输入类型\")\n",
        "        input_audio=gr.Audio(label=\"录音\",type=\"numpy\",source='microphone')\n",
        "        input_talk_text=gr.Textbox()\n",
        "        output_video2=gr.Video()\n",
        "        btn2 = gr.Button(value=\"生成视频\")\n",
        "        btn2.click(test2, inputs=[input_talk_text,input_audio,input_file_type], outputs=[output_video2],api_name='create_talk_face')\n",
        "      \n",
        "    # gr.Interface(fn=create_avatar, inputs=[create_avatar], outputs=[output_video1],\n",
        "    #       layout=\"vertical\")\n",
        "    # gr.Interface(fn=test, inputs=[input_audio,input_dropdown], outputs=[output_video2],\n",
        "    #       layout=\"vertical\")\n",
        "\n",
        "    demo.queue(concurrency_count=2)\n",
        "\n",
        "    demo.launch(\n",
        "                # server_name='0.0.0.0',\n",
        "                share=True,\n",
        "                debug=True,\n",
        "                max_threads=2,\n",
        "                show_error=True,\n",
        "                show_api=True\n",
        "                # file_directories=[root_path]\n",
        "                )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}